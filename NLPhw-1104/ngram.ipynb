{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace494b",
   "metadata": {},
   "source": [
    "# n元语言模型回退算法\n",
    "\n",
    "本次作业要求补全本笔记中的n元语言模型的采用Good-Turing折扣的Katz回退算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d9726",
   "metadata": {},
   "source": [
    "### 预处理\n",
    "\n",
    "首先创建一些预处理函数。\n",
    "\n",
    "引入必要的模块，定义些类型别名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0097797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools\n",
    "\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "Sentence = List[str]\n",
    "IntSentence = List[int]\n",
    "\n",
    "Corpus = List[Sentence]\n",
    "IntCorpus = List[IntSentence]\n",
    "\n",
    "Gram = Tuple[int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c02038",
   "metadata": {},
   "source": [
    "下面的函数用于将文本正则化并词元化。该函数会将所有英文文本转为小写，去除文本中所有的标点，简单起见将所有连续的数字用一个`N`代替，将形如`let's`的词组拆分为`let`和`'s`两个词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd05065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_splitor_pattern = re.compile(r\"[^a-zA-Z']+|(?=')\")\n",
    "_digit_pattern = re.compile(r\"\\d+\")\n",
    "def normaltokenize(corpus: List[str]) -> Corpus:\n",
    "    \"\"\"\n",
    "    Normalizes and tokenizes the sentences in `corpus`. Turns the letters into\n",
    "    lower case and removes all the non-alphadigit characters and splits the\n",
    "    sentence into words and added BOS and EOS marks.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of str\n",
    "\n",
    "    Return:\n",
    "        list of list of str where each inner list of str represents the word\n",
    "          sequence in a sentence from the original sentence list\n",
    "    \"\"\"\n",
    "\n",
    "    tokeneds = [ [\"<s>\"]\n",
    "               + list(\n",
    "                   filter(lambda tkn: len(tkn)>0,\n",
    "                       _splitor_pattern.split(\n",
    "                           _digit_pattern.sub(\"N\", stc.lower()))))\n",
    "               + [\"</s>\"]\n",
    "                    for stc in corpus\n",
    "               ]\n",
    "    return tokeneds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2768c1",
   "metadata": {},
   "source": [
    "接下来定义两个函数用来从训练语料中构建词表，并将句子中的单词从字符串表示转为整数索引表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4685897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vocabulary(corpus: Corpus) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Extracts the vocabulary from `corpus` and returns it as a mapping from the\n",
    "    word to index. The words will be sorted by the codepoint value.\n",
    "\n",
    "    Args:\n",
    "        corpus - list of list of str\n",
    "\n",
    "    Return:\n",
    "        dict like {str: int}\n",
    "    \"\"\"\n",
    "\n",
    "    vocabulary = set(itertools.chain.from_iterable(corpus))\n",
    "    vocabulary = dict(\n",
    "            map(lambda itm: (itm[1], itm[0]),\n",
    "                enumerate(\n",
    "                    sorted(vocabulary))))\n",
    "    return vocabulary\n",
    "\n",
    "def words_to_indices(vocabulary: Dict[str, int], sentence: Sentence) -> IntSentence:\n",
    "    \"\"\"\n",
    "    Convert sentence in words to sentence in word indices.\n",
    "\n",
    "    Args:\n",
    "        vocabulary - dict like {str: int}\n",
    "        sentence - list of str\n",
    "\n",
    "    Return:\n",
    "        list of int\n",
    "    \"\"\"\n",
    "\n",
    "    return list(map(lambda tkn: vocabulary.get(tkn, len(vocabulary)), sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265aba2b",
   "metadata": {},
   "source": [
    "接下来读入训练数据，将数据预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af69f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training set.\n",
      "Preprocessed training set.\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.train\") as f:\n",
    "    texts = list(map(lambda l: l.strip(), f.readlines()))\n",
    "\n",
    "print(\"Loaded training set.\")\n",
    "\n",
    "corpus = normaltokenize(texts)\n",
    "vocabulary = extract_vocabulary(corpus)\n",
    "corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            corpus))\n",
    "\n",
    "print(\"Preprocessed training set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a007b899",
   "metadata": {},
   "source": [
    "### 设计模型\n",
    "\n",
    "参照公式\n",
    "\n",
    "$$\n",
    "P_{\\text{bo}}(w_k | W_{k-n+1}^{k-1}) = \\begin{cases}\n",
    "    d(W_{k-n+1}^k) \\dfrac{C(W_{k-n+1}^k)}{C(W_{k-n+1}^{k-1})} &  C(W_{k-n+1}^k) > 0 \\\\\n",
    "    \\alpha(W_{k-n+1}^{k-1}) P_{\\text{bo}}(w_k | W_{k-n+2}^{k-1}) &  \\text{否则} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "实现n元语言模型及采用Good-Turing折扣的Katz回退算法。\n",
    "\n",
    "需要实现的功能包括：\n",
    "\n",
    "1. 统计各词组（gram）在训练语料中的频数\n",
    "2. 计算同频词组个数$N_r$\n",
    "3. 计算$d(W_{k-n+1}^k)$\n",
    "4. 计算$\\alpha(W_{k-n+1}^{k-1})$\n",
    "5. 根据公式计算回退概率\n",
    "6. 计算概率对数与困惑度（PPL）\n",
    "\n",
    "$d$与$\\alpha$如何计算可以参考作业文件中的算法说明以及[SRILM](http://www.speech.sri.com/projects/srilm/)的[`ngram-discount(7)`手册页](http://www.speech.sri.com/projects/srilm/manpages/ngram-discount.7.html)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea6708a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, vocab_size: int, n: int = 4):\n",
    "        \"\"\"\n",
    "        Constructs `n`-gram model with a `vocab_size`-size vocabulnextry.\n",
    "\n",
    "        Args:\n",
    "            vocab_size - int\n",
    "            n - int\n",
    "        \"\"\"\n",
    "\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.n: int = n\n",
    "\n",
    "        self.frequencies: List[Dict[Gram, int]] = [{} for _ in range(n)]\n",
    "        self.disfrequencies: List[Dict[Gram, int]] = [{} for _ in range(n)]\n",
    "        \n",
    "        self.ncounts: Dict[ Gram, Dict[int, int]] = {}\n",
    "        self.discount_threshold:int = 7\n",
    "        # we only need to return a float instead of a tuple, so I modified the code here\n",
    "        self._d: Dict[Gram, float] = {} \n",
    "        self._alpha: List[Dict[Gram, float]] = [{} for _ in range(n)]\n",
    "        self.eps = 4e-10\n",
    "\n",
    "    def learn(self, corpus: IntCorpus):\n",
    "        \"\"\"\n",
    "        Learns the parameters of the n-gram model.\n",
    "\n",
    "        Args:\n",
    "            corpus - list of list of int\n",
    "        \"\"\"\n",
    "        for stc in corpus:\n",
    "            for i in range(1, len(stc)+1):\n",
    "                for j in range(min(i, self.n)):\n",
    "                    # TODO: count the frequencies of the grams\n",
    "                    \n",
    "                    gram = tuple(stc[i - j - 1: i])\n",
    "                    if gram not in self.frequencies[j]:\n",
    "                        self.frequencies[j][gram] = 1\n",
    "                    else:\n",
    "                        self.frequencies[j][gram] += 1\n",
    "                        \n",
    "        for i in range(1, self.n):\n",
    "            grams = itertools.groupby(\n",
    "                    sorted(\n",
    "                        sorted(\n",
    "                            map(lambda itm: (itm[0][:-1], itm[1]),\n",
    "                                 self.frequencies[i].items()),\n",
    "                               key=(lambda itm: itm[1])),\n",
    "                        key=(lambda itm: itm[0])))\n",
    "            # TODO: calculates the value of $N_r$\n",
    "            \n",
    "            for key, elem in grams:\n",
    "                gram, num, l = key[0], key[1], len(list(elem))\n",
    "                if gram not in self.ncounts:\n",
    "                    self.ncounts[gram]={}\n",
    "                self.ncounts[gram][num]=l\n",
    "\n",
    "    def d(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the interpolation coefficient.\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        if gram not in self._d:\n",
    "            # TODO: calculates the value of $d'$\n",
    "\n",
    "            n = len(gram)-1\n",
    "            theta, r = self.discount_threshold, self.frequencies[n][gram]\n",
    "            gram1 = gram[:-1]\n",
    "            \n",
    "            if self.frequencies[n][gram] > theta or 1 not in self.ncounts[gram1]:\n",
    "                self._d[gram]=1\n",
    "            else:\n",
    "                self.key_check(gram1, theta + 1)\n",
    "                self.key_check(gram1, r + 1)  \n",
    "                divisor = self.ncounts[gram1][1] - (theta + 1) * self.ncounts[gram1][theta + 1]\n",
    "                lamda = self.ncounts[gram1][1]\n",
    "                if divisor != 0:\n",
    "                    lamda /= divisor\n",
    "                self._d[gram] = lamda * (r + 1) * self.ncounts[gram1][r + 1]/(r * self.ncounts[gram1][r]) + 1 - lamda\n",
    "        return self._d[gram]\n",
    "\n",
    "    def key_check(self, gram, p):\n",
    "        if p not in self.ncounts[gram]:\n",
    "            self.ncounts[gram][p] = 0\n",
    "        \n",
    "    def alpha(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the back-off weight alpha(`gram`)\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)\n",
    "        if gram not in self._alpha[n]:\n",
    "            if gram in self.frequencies[n-1]:\n",
    "                # TODO: calculates the value of $\\alpha$\n",
    "                \n",
    "                numerator = 1.\n",
    "                denominator = 1.\n",
    "                for key in self.frequencies[n]:\n",
    "                    if key[:-1] == gram:\n",
    "                        numerator -= self[key]\n",
    "                        denominator -= self[key[1:]]\n",
    "                self._alpha[n][gram] = numerator/denominator\n",
    "                \n",
    "            else:\n",
    "                self._alpha[n][gram] = 1.\n",
    "        return self._alpha[n][gram]\n",
    "\n",
    "    def __getitem__(self, gram: Gram) -> float:\n",
    "        \"\"\"\n",
    "        Calculates smoothed conditional probability P(`gram[-1]`|`gram[:-1]`).\n",
    "\n",
    "        Args:\n",
    "            gram - tuple of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        n = len(gram)-1\n",
    "        if gram not in self.disfrequencies[n]:\n",
    "            if n>0:\n",
    "                # TODO: calculates the smoothed probability value according to the formulae\n",
    "                \n",
    "                gram1 = tuple(list(gram)[:-1])\n",
    "                gram2 = tuple(list(gram)[1:])\n",
    "                if gram in self.frequencies[n]:\n",
    "                    self.disfrequencies[n][gram] = self.d(gram) * self.frequencies[n][gram] / self.frequencies[n-1][gram1]\n",
    "                else:\n",
    "                    alpha = self.alpha(gram1) \n",
    "                    self.disfrequencies[n][gram] = alpha * self[gram2]\n",
    "                \n",
    "            else: # uni-gram, n == 0\n",
    "                self.disfrequencies[n][gram] = \\\n",
    "                self.frequencies[n].get(gram, self.eps)/float(len(self.frequencies[0]))\n",
    "        return self.disfrequencies[n][gram]\n",
    "\n",
    "    def log_prob(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the log probability of the given sentence. Assumes that the\n",
    "        first token is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        log_prob = 0.\n",
    "        for i in range(2, len(sentence) + 1):\n",
    "            # TODO: calculates the log probability\n",
    "            \n",
    "            if i < 4:\n",
    "                result = max(self.eps, self[tuple(sentence[:i])])\n",
    "            else:\n",
    "                result = max(self.eps, self[tuple(sentence[i-4: i])])\n",
    "            log_prob += math.log2(result)\n",
    "        return log_prob / (len(sentence) - 1)\n",
    "\n",
    "    def ppl(self, sentence: IntSentence) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the PPL of the given sentence. Assumes that the first token\n",
    "        is always \"<s>\".\n",
    "\n",
    "        Args:\n",
    "            sentence: list of int\n",
    "\n",
    "        Return:\n",
    "            float\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: calculates the PPL\n",
    "        return math.pow(2, - self.log_prob(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbcf678",
   "metadata": {},
   "source": [
    "### 训练与测试\n",
    "\n",
    "现在数据与模型均已齐备，可以训练并测试了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd706656",
   "metadata": {},
   "source": [
    "训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5f988b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dumped model.\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "model = NGramModel(len(vocabulary))\n",
    "model.learn(corpus)\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pkl.dump(vocabulary, f)\n",
    "    pkl.dump(model, f)\n",
    "\n",
    "print(\"Dumped model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f97ca6",
   "metadata": {},
   "source": [
    "在测试集上测试计算困惑度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf8b1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model.\n",
      "10463.588123518293\n",
      "15719.586004121687\n",
      "25947.193401888333\n",
      "10765.854341282022\n",
      "10888.17818896432\n",
      "186.47650358820258\n",
      "3042934.577436564\n",
      "584.2650073977819\n",
      "24209.62831696758\n",
      "9863.45979705811\n",
      "459.550274250479\n",
      "1773.8265452528894\n",
      "510807.43432061607\n",
      "76747.84191048461\n",
      "46494.10449870692\n",
      "92232.66964067344\n",
      "7298.755805777088\n",
      "147502.42534418983\n",
      "23272.592766855698\n",
      "858.6720071442478\n",
      "20621.969937023405\n",
      "44027.413229212536\n",
      "23693.117233963236\n",
      "154063.44055061313\n",
      "5351.793336398942\n",
      "45423.91830330898\n",
      "28298.23471980987\n",
      "316.0490231609363\n",
      "9993.065806659655\n",
      "3802.1243512096735\n",
      "32219.030398408362\n",
      "103473.80581686196\n",
      "16920.847567551627\n",
      "3232.691373799455\n",
      "78796.2940148679\n",
      "9137.53220517289\n",
      "77712.89368209135\n",
      "32737.0744006341\n",
      "133480.32399795204\n",
      "187527.1916346902\n",
      "20856.27728365577\n",
      "92100.15033858709\n",
      "5622.785714053623\n",
      "187956.49051349924\n",
      "35611.62545809663\n",
      "580822.0551989726\n",
      "38525.611875233044\n",
      "8492.83100577115\n",
      "89916.47839868657\n",
      "197991.7719375021\n",
      "Avg:  126554.711390855\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    vocabulary = pkl.load(f)\n",
    "    model = pkl.load(f)\n",
    "print(\"Loaded model.\")\n",
    "\n",
    "with open(\"data/news.2007.en.shuffled.deduped.test\") as f:\n",
    "    test_set = list(map(lambda l: l.strip(), f.readlines()))\n",
    "test_corpus = normaltokenize(test_set)\n",
    "test_corpus = list(\n",
    "        map(functools.partial(words_to_indices, vocabulary),\n",
    "            test_corpus))\n",
    "ppls = []\n",
    "for t in test_corpus:\n",
    "    ppls.append(model.ppl(t))\n",
    "    print(ppls[-1])\n",
    "print(\"Avg: \", sum(ppls)/len(ppls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd6e8f",
   "metadata": {},
   "source": [
    "代码简要说明：\n",
    "\n",
    "1.词频的计算，直接统计即可。需要特别注意的是，当第一次遇到某个gram时，相应的字典键值还未创建，这时候需要根据当前gram创建对应键值，而以后再遇到同样的gram时只需直接增加对应key的value即可。\n",
    "\n",
    "2.ncounts的计算：由于本代码给出的Nr定义是在特定前缀条件下，出现恰为r次的n元组的个数，按照此定义，我们只需要在代码给出嵌套排序的基础上，对于返回的迭代器进行操作。通过阅读代码可以发现，返回的迭代器中，按照groupby操作已经将所有具有相同前缀的gram放在了一起，并且，对于所有这些相同前缀的gram，会按照他们的词频升序排序，那么我们为了统计特定前缀条件下的Nr，只需要遍历迭代器，然后将其中的「相同前缀且相同词频」的gram取出，用len函数统计每组的个数，然后将结果保存在ncounts结构中即可\n",
    "\n",
    "3-5.d、alpha、probability的计算：直接按照作业说明中给出的公式进行计算即可。为了方便处理回退时遇到的一些未定义的ncounts的问题，我定义了一个检查函数key_check，用于处理对应gram在定长下的ncounts是否已经进行过统计。如果没有进行统计，那么应该将对应的ncount置零。\n",
    "\n",
    "6-7.概率和困惑度的计算：直接按照ppt中的定义即可。实际上在计算时可能遇到回退后概率非常小，这时候我们只需要根据给定的self.eps设定的阈值，取回退概率和阈值中的较大值进行计算即可。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
